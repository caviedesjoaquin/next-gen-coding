{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e834c658",
      "metadata": {
        "id": "e834c658"
      },
      "source": [
        "# Reto 1 - Sistema RAG usando IA Generativa\n",
        "## 1. Instalación de dependencias y inicialización del entorno para generar las respuestas del LLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instalamos langchain\n",
        "!pip install langchain_openai -q"
      ],
      "metadata": {
        "id": "uSMq5li09izP"
      },
      "id": "uSMq5li09izP",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el TOKEN de OpenAI para poder ejecutar prompts\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('OpenAI_Key')"
      ],
      "metadata": {
        "id": "4oUnsAvy9Nj7"
      },
      "id": "4oUnsAvy9Nj7",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "571b4acd",
      "metadata": {
        "id": "571b4acd"
      },
      "outputs": [],
      "source": [
        "# Importamos las librerias para ejecutar los prompts\n",
        "from langchain_openai import ChatOpenAI\n",
        "# Inicializamos el modelo LangChain OpenAI\n",
        "# Seleccionamos la temperatura de 0 para mantener el modelo lo mas\n",
        "# deterministico posible\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    api_key=api_key,\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Zeroshot con metodlogía CoT sin espeicificar la implementación para obetener un panorama general de la información contenida en el LLM"
      ],
      "metadata": {
        "id": "rsoqjwr7im2n"
      },
      "id": "rsoqjwr7im2n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "611f3df3",
      "metadata": {
        "id": "611f3df3"
      },
      "outputs": [],
      "source": [
        "# Zero-shot concadena de pensamiento simple\n",
        "zero_shot_cot = \"\"\"\n",
        "Actúa como un arquitecto senior de software y piensa paso a paso (Chain of Thought)\n",
        "para diseñar el ciclo de desarrollo de un sistema RAG usando langchain y OpenAI.\n",
        "\n",
        "Describe el razonamiento en pasos enumerados.\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d59ba72c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d59ba72c",
        "outputId": "202a2342-6cf2-4b11-92a4-ffbe827c08d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Claro, aquí tienes un enfoque paso a paso para diseñar el ciclo de desarrollo de un sistema RAG (Retrieval-Augmented Generation) utilizando LangChain y OpenAI:\n",
            "\n",
            "### Paso 1: Definición de Requisitos\n",
            "- **Objetivo del Sistema**: Definir claramente qué problema resolverá el sistema RAG. Por ejemplo, ¿será un asistente virtual, un sistema de recomendación, o un generador de contenido?\n",
            "- **Usuarios y Casos de Uso**: Identificar quiénes serán los usuarios finales y cuáles son los casos de uso específicos que el sistema debe abordar.\n",
            "\n",
            "### Paso 2: Investigación y Selección de Tecnologías\n",
            "- **Evaluar LangChain**: Investigar las capacidades de LangChain para la integración de modelos de lenguaje y sistemas de recuperación de información.\n",
            "- **Seleccionar el Modelo de OpenAI**: Decidir qué modelo de OpenAI se utilizará (por ejemplo, GPT-3.5, GPT-4) en función de los requisitos de rendimiento y costo.\n",
            "\n",
            "### Paso 3: Diseño de la Arquitectura\n",
            "- **Componentes del Sistema**: Definir los componentes principales del sistema, que pueden incluir:\n",
            "  - **Módulo de Recuperación**: Para buscar y recuperar información relevante de una base de datos o fuente de datos.\n",
            "  - **Módulo de Generación**: Para generar respuestas o contenido utilizando el modelo de OpenAI.\n",
            "  - **Interfaz de Usuario**: Para la interacción con los usuarios finales.\n",
            "- **Flujo de Datos**: Diseñar cómo fluirán los datos entre los componentes, desde la entrada del usuario hasta la salida generada.\n",
            "\n",
            "### Paso 4: Prototipado\n",
            "- **Desarrollo de un Prototipo Inicial**: Crear un prototipo básico que integre LangChain y OpenAI para validar la idea. Esto puede incluir una interfaz simple y una funcionalidad básica de recuperación y generación.\n",
            "- **Pruebas Iniciales**: Realizar pruebas con usuarios para obtener retroalimentación sobre la funcionalidad y la usabilidad del prototipo.\n",
            "\n",
            "### Paso 5: Implementación\n",
            "- **Desarrollo Iterativo**: Implementar el sistema en iteraciones, comenzando con las características más críticas y luego agregando funcionalidades adicionales.\n",
            "- **Integración de LangChain**: Configurar LangChain para manejar la recuperación de información y la generación de texto, asegurando que se utilicen las mejores prácticas de implementación.\n",
            "\n",
            "### Paso 6: Pruebas y Validación\n",
            "- **Pruebas Unitarias y de Integración**: Desarrollar pruebas para cada componente del sistema para asegurar que funcionen correctamente de manera aislada y en conjunto.\n",
            "- **Pruebas de Usuario**: Realizar pruebas con usuarios reales para validar que el sistema cumple con los requisitos y es fácil de usar.\n",
            "\n",
            "### Paso 7: Despliegue\n",
            "- **Preparación para el Despliegue**: Configurar el entorno de producción, asegurando que todos los componentes estén correctamente configurados y optimizados.\n",
            "- **Despliegue del Sistema**: Lanzar el sistema RAG en un entorno de producción, asegurando que se sigan las mejores prácticas de seguridad y rendimiento.\n",
            "\n",
            "### Paso 8: Monitoreo y Mantenimiento\n",
            "- **Monitoreo del Rendimiento**: Implementar herramientas de monitoreo para rastrear el rendimiento del sistema y la satisfacción del usuario.\n",
            "- **Mantenimiento y Actualizaciones**: Establecer un plan para el mantenimiento continuo del sistema, incluyendo actualizaciones de modelos, mejoras de funcionalidad y corrección de errores.\n",
            "\n",
            "### Paso 9: Recopilación de Retroalimentación y Mejora Continua\n",
            "- **Recopilación de Retroalimentación**: Establecer canales para que los usuarios proporcionen retroalimentación sobre el sistema.\n",
            "- **Iteración Basada en Retroalimentación**: Utilizar la retroalimentación para realizar mejoras continuas en el sistema, ajustando tanto la recuperación como la generación de contenido.\n",
            "\n",
            "### Paso 10: Documentación\n",
            "- **Documentación del Sistema**: Crear documentación técnica y de usuario que explique cómo funciona el sistema, cómo se puede utilizar y cómo se puede mantener.\n",
            "- **Capacitación**: Proporcionar capacitación a los usuarios finales y al equipo de soporte sobre el uso y mantenimiento del sistema.\n",
            "\n",
            "Este enfoque paso a paso proporciona una guía clara para el desarrollo de un sistema RAG utilizando LangChain y OpenAI, asegurando que se aborden todos los aspectos críticos del ciclo de vida del desarrollo de software.\n"
          ]
        }
      ],
      "source": [
        "zero_shot_cot_response = llm.invoke(zero_shot_cot)\n",
        "print(zero_shot_cot_response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observaciones:\n",
        "\n",
        "\n",
        "*   El modelo nos presento la respuesta de manera general, mencinando los pasos a seguir para la implementación.\n",
        "*   Menciono las metodologías que se le solictaron, pero no incluyó de manera específica como usarlas (no se le solicto al modelo)\n",
        "*   La respuesta demuestra que el modelo tiene la información necesaria, ya que en ningún momento nos mencion que desconocía alguna de las tecnlogías solicitadas.\n"
      ],
      "metadata": {
        "id": "22eI9Wdfjzc1"
      },
      "id": "22eI9Wdfjzc1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Fewshot con CoT para obetener los pasos generales para la implementación del sistema RAG"
      ],
      "metadata": {
        "id": "fDfyIEIYiz5T"
      },
      "id": "fDfyIEIYiz5T"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "93760f8a",
      "metadata": {
        "id": "93760f8a"
      },
      "outputs": [],
      "source": [
        "# Few-shot con Cadena de Pensamiento simple\n",
        "few_shot_cot = \"\"\"\n",
        "Actúa como un arquitecto senior de software y piensa paso a paso (Chain of Thought)\n",
        "para diseñar el ciclo de desarrollo de un sistema RAG usando langchain y OpenAI.\n",
        "\n",
        "Describe el razonamiento en pasos enumerados.\n",
        "\n",
        "El diseño debe de incluir las siguientes funcionalidades:\n",
        "Paso 1: Extracción de texto de archivos pdf y txt usando langchain\n",
        "Paso 2: División de texto en fragmentos (chunking) usando langchain\n",
        "Paso 3: Creación de embeddings\n",
        "Paso 4: Almacenamiento vectorial en ChromaDB\n",
        "Paso 5: Sistema de recuperación de información relevante\n",
        "Paso 6: Generar los prompts utilizando los fragmentos de textos obtenidos en el Paso 5.\n",
        "Paso 7: Generación de respuestas usando OpenAI GPT-4 usando langchain\n",
        "\n",
        "Incluye en la respuesta:\n",
        "1. Las depenencias necesarias para ejecutar el código.\n",
        "2. Código de ejemplo para cada paso.\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "a15b29ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a15b29ab",
        "outputId": "0684dff6-db19-41a1-fa5e-1e70ab517258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Para diseñar un sistema de Recuperación de Respuestas Generativas (RAG) utilizando LangChain y OpenAI, seguiremos un enfoque paso a paso. A continuación, se describen los pasos necesarios, las dependencias requeridas y ejemplos de código para cada uno de ellos.\n",
            "\n",
            "### Dependencias necesarias\n",
            "\n",
            "Antes de comenzar, asegúrate de tener instaladas las siguientes bibliotecas:\n",
            "\n",
            "```bash\n",
            "pip install langchain openai chromadb PyPDF2\n",
            "```\n",
            "\n",
            "### Paso 1: Extracción de texto de archivos PDF y TXT usando LangChain\n",
            "\n",
            "**Razonamiento**: Necesitamos extraer texto de diferentes formatos de archivo para poder procesarlo posteriormente.\n",
            "\n",
            "**Código de ejemplo**:\n",
            "\n",
            "```python\n",
            "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
            "\n",
            "def load_documents(file_paths):\n",
            "    documents = []\n",
            "    for file_path in file_paths:\n",
            "        if file_path.endswith('.pdf'):\n",
            "            loader = PyPDFLoader(file_path)\n",
            "            documents.extend(loader.load())\n",
            "        elif file_path.endswith('.txt'):\n",
            "            loader = TextLoader(file_path)\n",
            "            documents.extend(loader.load())\n",
            "    return documents\n",
            "\n",
            "# Ejemplo de uso\n",
            "file_paths = ['document1.pdf', 'document2.txt']\n",
            "documents = load_documents(file_paths)\n",
            "```\n",
            "\n",
            "### Paso 2: División de texto en fragmentos (chunking) usando LangChain\n",
            "\n",
            "**Razonamiento**: Para facilitar la búsqueda y recuperación, dividimos el texto en fragmentos más pequeños.\n",
            "\n",
            "**Código de ejemplo**:\n",
            "\n",
            "```python\n",
            "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
            "\n",
            "def split_documents(documents):\n",
            "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
            "    chunks = text_splitter.split_documents(documents)\n",
            "    return chunks\n",
            "\n",
            "# Ejemplo de uso\n",
            "chunks = split_documents(documents)\n",
            "```\n",
            "\n",
            "### Paso 3: Creación de embeddings\n",
            "\n",
            "**Razonamiento**: Convertimos los fragmentos de texto en embeddings para facilitar la búsqueda semántica.\n",
            "\n",
            "**Código de ejemplo**:\n",
            "\n",
            "```python\n",
            "from langchain.embeddings import OpenAIEmbeddings\n",
            "\n",
            "def create_embeddings(chunks):\n",
            "    embeddings = OpenAIEmbeddings()\n",
            "    chunk_embeddings = [embeddings.embed(chunk.page_content) for chunk in chunks]\n",
            "    return chunk_embeddings\n",
            "\n",
            "# Ejemplo de uso\n",
            "chunk_embeddings = create_embeddings(chunks)\n",
            "```\n",
            "\n",
            "### Paso 4: Almacenamiento vectorial en ChromaDB\n",
            "\n",
            "**Razonamiento**: Almacenamos los embeddings en una base de datos vectorial para una recuperación eficiente.\n",
            "\n",
            "**Código de ejemplo**:\n",
            "\n",
            "```python\n",
            "import chromadb\n",
            "\n",
            "def store_embeddings_in_chromadb(chunks, chunk_embeddings):\n",
            "    client = chromadb.Client()\n",
            "    collection = client.create_collection(\"documents\")\n",
            "    \n",
            "    for chunk, embedding in zip(chunks, chunk_embeddings):\n",
            "        collection.add(\n",
            "            documents=[chunk.page_content],\n",
            "            embeddings=[embedding],\n",
            "            metadatas=[{\"source\": chunk.metadata['source']}]\n",
            "        )\n",
            "\n",
            "# Ejemplo de uso\n",
            "store_embeddings_in_chromadb(chunks, chunk_embeddings)\n",
            "```\n",
            "\n",
            "### Paso 5: Sistema de recuperación de información relevante\n",
            "\n",
            "**Razonamiento**: Implementamos un sistema que permite recuperar los fragmentos más relevantes basados en una consulta.\n",
            "\n",
            "**Código de ejemplo**:\n",
            "\n",
            "```python\n",
            "def retrieve_relevant_chunks(query):\n",
            "    client = chromadb.Client()\n",
            "    collection = client.get_collection(\"documents\")\n",
            "    results = collection.query(\n",
            "        query_embeddings=[OpenAIEmbeddings().embed(query)],\n",
            "        n_results=5\n",
            "    )\n",
            "    return results['documents']\n",
            "\n",
            "# Ejemplo de uso\n",
            "query = \"¿Cuál es el tema principal del documento?\"\n",
            "relevant_chunks = retrieve_relevant_chunks(query)\n",
            "```\n",
            "\n",
            "### Paso 6: Generar los prompts utilizando los fragmentos de textos obtenidos en el Paso 5\n",
            "\n",
            "**Razonamiento**: Creamos un prompt que combine los fragmentos relevantes para enviar a GPT-4.\n",
            "\n",
            "**Código de ejemplo**:\n",
            "\n",
            "```python\n",
            "def generate_prompt(relevant_chunks):\n",
            "    prompt = \"Aquí hay algunos fragmentos relevantes:\\n\"\n",
            "    for chunk in relevant_chunks:\n",
            "        prompt += f\"- {chunk}\\n\"\n",
            "    prompt += \"Por favor, proporciona una respuesta basada en estos fragmentos.\"\n",
            "    return prompt\n",
            "\n",
            "# Ejemplo de uso\n",
            "prompt = generate_prompt(relevant_chunks)\n",
            "```\n",
            "\n",
            "### Paso 7: Generación de respuestas usando OpenAI GPT-4 usando LangChain\n",
            "\n",
            "**Razonamiento**: Finalmente, utilizamos el modelo GPT-4 para generar una respuesta basada en el prompt creado.\n",
            "\n",
            "**Código de ejemplo**:\n",
            "\n",
            "```python\n",
            "from langchain.llms import OpenAI\n",
            "\n",
            "def generate_response(prompt):\n",
            "    llm = OpenAI(model_name=\"gpt-4\")\n",
            "    response = llm(prompt)\n",
            "    return response\n",
            "\n",
            "# Ejemplo de uso\n",
            "response = generate_response(prompt)\n",
            "print(response)\n",
            "```\n",
            "\n",
            "### Resumen\n",
            "\n",
            "Hemos diseñado un ciclo de desarrollo para un sistema RAG utilizando LangChain y OpenAI. Cada paso se ha desglosado con su respectivo código de ejemplo, lo que permite implementar un sistema que extrae, procesa y genera respuestas a partir de documentos en formato PDF y TXT. Este enfoque modular facilita la comprensión y la implementación de cada componente del sistema.\n"
          ]
        }
      ],
      "source": [
        "few_shot_cot_response = llm.invoke(few_shot_cot)\n",
        "print(few_shot_cot_response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Fewshot con metodología CoT y CREATE para la crecaión de un promppt mas especifico y con los requerimientos especificos para el sistema RAG"
      ],
      "metadata": {
        "id": "1U741FPcjHDj"
      },
      "id": "1U741FPcjHDj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shot con Cadena de Pensamiento utilizando tecnología C.R.E.A.T.E.\n",
        "few_shot_cot_create = \"\"\"\n",
        "<instruction>\n",
        "Actúa como un arquitecto senior de software de Python especializado en sistemas RAG y\n",
        "ciencia de datos, y amplia experiencia en implementaciones en Github Actions.\n",
        "</instruction>\n",
        "\n",
        "<context>\n",
        "Estoy trabajando en la implementación de un sistema RAG (Retrieval-Augmented Generation)\n",
        "utilizando langchain y OpenAI GPT-4, para la consulta de documementos internos de una empresa.\n",
        "</context>\n",
        "\n",
        "<requirements>\n",
        "Crear un sistema RAG que:\n",
        "- Utilice la tecnica de Chain of thought para diseñar el ciclo de desarrollo.\n",
        "- Permite cargar o indexar por lo menos dos documentos tipo txt o pdf\n",
        "- Los documentos solo se almanecarn una vez en ChromaDB.\n",
        "- La base de datos de ChromaDB se almacenara en un archivo en el directorio y unicamente\n",
        "se volvera a cargar en memoria cuando el usuario haga una pregunta.\n",
        "- Reciba una pregunta del usuario.\n",
        "- Utilice el modelo generativo de OpenAI GPT-4 para generar una respuesta, apoyandose\n",
        "en la información recuperada de los documentos.\n",
        "- Implemente una función que reciba una pregunta del usuario, recupere los fragmentos\n",
        "mas relevantes de los documentos y pase la pregunta y los fragmentos al modelo generativo.\n",
        "</requirements>\n",
        "\n",
        "<specifications>\n",
        "- Debe de usar langchain y OpenAI GPT-4\n",
        "- Debe de usar la tecnica de Chain of thought para diseñar el ciclo de desarrollo.\n",
        "- Debe usar ChromaDB para almacenar los fragmentos de texto.\n",
        "- Debe de implementar una función que reciba una pregunta del usuario, recupere los fragmentos\n",
        "mas relevantes de los documentos y pase la pregunta y los fragmentos al modelo generativo.\n",
        "- Debe de incluir las dependencias necesarias para ejecutar el código.\n",
        "- Debe de incluir código de ejemplo para cada paso.\n",
        "- Debe de tomar en cuenta que se implementara en GitHub Actions.\n",
        "- Debe de incluir los siguientes pasos:\n",
        "    Paso 1: Extracción de texto de archivos pdf y txt usando langchain\n",
        "    Paso 2: División de texto en fragmentos (chunking) usando langchain\n",
        "    Paso 3: Creación de embeddings\n",
        "    Paso 4: Almacenamiento vectorial en ChromaDB si la base de datos no existe en el directorio.\n",
        "    Paso 5: Si la base de datos existe en el directorio, se volvera a cargar en memoria.\n",
        "    Paso 6: Sistema de recuperación de información relevante\n",
        "    Paso 7: Generar los prompts utilizando los fragmentos de textos obtenidos en el Paso 5.\n",
        "    Paso 8: Generación de respuestas usando OpenAI GPT-4 usando langchain\n",
        "- Debe de describir el proceso de diseño en pasos enumerados.\n",
        "</specifications>\n",
        "\n",
        "<artifacts>\n",
        "Genera en orden especifico:\n",
        "1. El notebook the python cumpliendo con las especificaciones descritas en la seccion specifications.\n",
        "2. Los pasos necesarios para implementar el notebook en GitHub Actions.\n",
        "3. Los pasos necesarios para ejecutar el notebook en GitHub Actions.\n",
        "4. Los archivos necesarios para ejecutar el notebook en GitHub Actions incluyendo el archivo requirements y las instrucciones para ejecutarlo.\n",
        "5. Las instrucciones necesarias para ejecutar el notebook en GitHub Actions.\n",
        "5. Crea documentacion del notebook para Github Pages.\n",
        "6. Crea un archivo README.md para Github Actions.\n",
        "</artifacts>\n",
        "\n",
        "<testing>\n",
        "- Implementa pruebas unitarias y de integracion para el notebook con pytest.\n",
        "- Implementa pruebas de ejecución para el notebook.\n",
        "</testing>\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "IWlIPhfYHpSC"
      },
      "id": "IWlIPhfYHpSC",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_cot_create_response = llm.invoke(few_shot_cot_create)\n",
        "print(few_shot_cot_create_response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAMcKEmxgYAV",
        "outputId": "0e55ba2b-6750-412f-d0f5-a9b1610f3baa"
      },
      "id": "tAMcKEmxgYAV",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A continuación, se presenta un diseño detallado para implementar un sistema RAG utilizando Langchain y OpenAI GPT-4, siguiendo las especificaciones y requisitos proporcionados. Este diseño incluye un notebook de Python, pasos para implementar en GitHub Actions, y documentación necesaria.\n",
            "\n",
            "### 1. Notebook de Python\n",
            "\n",
            "```python\n",
            "# notebook_rag.py\n",
            "\n",
            "# Paso 1: Importar las librerías necesarias\n",
            "import os\n",
            "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
            "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
            "from langchain.embeddings import OpenAIEmbeddings\n",
            "from langchain.vectorstores import Chroma\n",
            "from langchain.chains import RetrievalQA\n",
            "from langchain.llms import OpenAI\n",
            "import pickle\n",
            "\n",
            "# Paso 2: Función para cargar documentos\n",
            "def load_documents(file_paths):\n",
            "    documents = []\n",
            "    for file_path in file_paths:\n",
            "        if file_path.endswith('.pdf'):\n",
            "            loader = PyPDFLoader(file_path)\n",
            "        elif file_path.endswith('.txt'):\n",
            "            loader = TextLoader(file_path)\n",
            "        else:\n",
            "            continue\n",
            "        documents.extend(loader.load())\n",
            "    return documents\n",
            "\n",
            "# Paso 3: Función para dividir texto en fragmentos\n",
            "def chunk_documents(documents):\n",
            "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
            "    return text_splitter.split_documents(documents)\n",
            "\n",
            "# Paso 4: Función para crear embeddings\n",
            "def create_embeddings(chunks):\n",
            "    embeddings = OpenAIEmbeddings()\n",
            "    return embeddings.embed_documents(chunks)\n",
            "\n",
            "# Paso 5: Función para almacenar en ChromaDB\n",
            "def store_in_chromadb(embeddings, db_path='chromadb.pkl'):\n",
            "    if not os.path.exists(db_path):\n",
            "        vector_store = Chroma.from_embeddings(embeddings)\n",
            "        with open(db_path, 'wb') as f:\n",
            "            pickle.dump(vector_store, f)\n",
            "    else:\n",
            "        with open(db_path, 'rb') as f:\n",
            "            vector_store = pickle.load(f)\n",
            "    return vector_store\n",
            "\n",
            "# Paso 6: Función para recuperar información relevante\n",
            "def retrieve_relevant_info(question, vector_store):\n",
            "    retriever = vector_store.as_retriever()\n",
            "    return retriever.get_relevant_documents(question)\n",
            "\n",
            "# Paso 7: Generar prompts\n",
            "def generate_prompt(question, relevant_docs):\n",
            "    context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
            "    return f\"Context: {context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
            "\n",
            "# Paso 8: Generar respuesta usando GPT-4\n",
            "def generate_answer(prompt):\n",
            "    llm = OpenAI(model_name=\"gpt-4\")\n",
            "    return llm(prompt)\n",
            "\n",
            "# Función principal\n",
            "def main(file_paths, question):\n",
            "    documents = load_documents(file_paths)\n",
            "    chunks = chunk_documents(documents)\n",
            "    embeddings = create_embeddings(chunks)\n",
            "    vector_store = store_in_chromadb(embeddings)\n",
            "    relevant_docs = retrieve_relevant_info(question, vector_store)\n",
            "    prompt = generate_prompt(question, relevant_docs)\n",
            "    answer = generate_answer(prompt)\n",
            "    return answer\n",
            "\n",
            "# Ejemplo de uso\n",
            "if __name__ == \"__main__\":\n",
            "    file_paths = ['document1.pdf', 'document2.txt']  # Cambiar por los nombres de los archivos\n",
            "    question = \"¿Cuál es el objetivo del proyecto?\"\n",
            "    answer = main(file_paths, question)\n",
            "    print(answer)\n",
            "```\n",
            "\n",
            "### 2. Pasos para implementar el notebook en GitHub Actions\n",
            "\n",
            "1. Crear un repositorio en GitHub.\n",
            "2. Subir el archivo `notebook_rag.py` al repositorio.\n",
            "3. Crear un archivo `requirements.txt` con las dependencias necesarias.\n",
            "4. Crear un archivo de configuración para GitHub Actions.\n",
            "\n",
            "### 3. Pasos para ejecutar el notebook en GitHub Actions\n",
            "\n",
            "1. Crear un directorio `.github/workflows` en el repositorio.\n",
            "2. Crear un archivo `python-app.yml` dentro de este directorio con la configuración de GitHub Actions.\n",
            "\n",
            "```yaml\n",
            "# .github/workflows/python-app.yml\n",
            "name: Python application\n",
            "\n",
            "on: [push]\n",
            "\n",
            "jobs:\n",
            "  build:\n",
            "    runs-on: ubuntu-latest\n",
            "\n",
            "    steps:\n",
            "    - uses: actions/checkout@v2\n",
            "    - name: Set up Python\n",
            "      uses: actions/setup-python@v2\n",
            "      with:\n",
            "        python-version: '3.8'\n",
            "    - name: Install dependencies\n",
            "      run: |\n",
            "        python -m pip install --upgrade pip\n",
            "        pip install -r requirements.txt\n",
            "    - name: Run the notebook\n",
            "      run: |\n",
            "        python notebook_rag.py\n",
            "```\n",
            "\n",
            "### 4. Archivos necesarios para ejecutar el notebook en GitHub Actions\n",
            "\n",
            "**requirements.txt**\n",
            "```plaintext\n",
            "langchain\n",
            "openai\n",
            "chromadb\n",
            "pickle5\n",
            "```\n",
            "\n",
            "### 5. Instrucciones para ejecutar el notebook en GitHub Actions\n",
            "\n",
            "1. Hacer un commit y push de los cambios al repositorio.\n",
            "2. Ir a la pestaña \"Actions\" en GitHub para ver la ejecución del flujo de trabajo.\n",
            "3. Verificar los logs para asegurarse de que el notebook se ejecutó correctamente.\n",
            "\n",
            "### 6. Documentación del notebook para GitHub Pages\n",
            "\n",
            "Crear un archivo `docs/index.md` con la siguiente estructura:\n",
            "\n",
            "```markdown\n",
            "# Documentación del Sistema RAG\n",
            "\n",
            "## Introducción\n",
            "Este sistema utiliza Langchain y OpenAI GPT-4 para responder preguntas basadas en documentos internos.\n",
            "\n",
            "## Instalación\n",
            "Para instalar las dependencias, ejecute:\n",
            "```\n",
            "pip install -r requirements.txt\n",
            "```\n",
            "\n",
            "## Uso\n",
            "Ejecute el script `notebook_rag.py` y proporcione los archivos y la pregunta.\n",
            "\n",
            "## Contribuciones\n",
            "Las contribuciones son bienvenidas. Por favor, abra un issue o un pull request.\n",
            "```\n",
            "\n",
            "### 7. Archivo README.md para GitHub Actions\n",
            "\n",
            "```markdown\n",
            "# Sistema RAG\n",
            "\n",
            "Este repositorio contiene un sistema de generación aumentada por recuperación (RAG) utilizando Langchain y OpenAI GPT-4.\n",
            "\n",
            "## Requisitos\n",
            "- Python 3.8 o superior\n",
            "- Dependencias en `requirements.txt`\n",
            "\n",
            "## Ejecución\n",
            "Para ejecutar el sistema, simplemente haga un push al repositorio y GitHub Actions se encargará de la ejecución.\n",
            "\n",
            "## Documentación\n",
            "La documentación está disponible en [GitHub Pages](URL_DE_GITHUB_PAGES).\n",
            "```\n",
            "\n",
            "### 8. Pruebas unitarias y de integración\n",
            "\n",
            "Crear un archivo `test_notebook.py` para las pruebas:\n",
            "\n",
            "```python\n",
            "# test_notebook.py\n",
            "import pytest\n",
            "from notebook_rag import load_documents, chunk_documents, create_embeddings\n",
            "\n",
            "def test_load_documents():\n",
            "    docs = load_documents(['test_document.txt'])\n",
            "    assert len(docs) > 0\n",
            "\n",
            "def test_chunk_documents():\n",
            "    docs = load_documents(['test_document.txt'])\n",
            "    chunks = chunk_documents(docs)\n",
            "    assert len(chunks) > 0\n",
            "\n",
            "def test_create_embeddings():\n",
            "    docs = load_documents(['test_document.txt'])\n",
            "    chunks = chunk_documents(docs)\n",
            "    embeddings = create_embeddings(chunks)\n",
            "    assert len(embeddings) > 0\n",
            "```\n",
            "\n",
            "### 9. Instrucciones para ejecutar las pruebas\n",
            "\n",
            "1. Instalar pytest:\n",
            "   ```bash\n",
            "   pip install pytest\n",
            "   ```\n",
            "2. Ejecutar las pruebas:\n",
            "   ```bash\n",
            "   pytest test_notebook.py\n",
            "   ```\n",
            "\n",
            "Con estos pasos y archivos, tendrás un sistema RAG completamente funcional, listo para ser implementado y probado en GitHub Actions.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "intro",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}